{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9137f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4976dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.llama.modeling_llama import (\n",
    "    apply_rotary_pos_emb,\n",
    "    repeat_kv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c094967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"openai-community/gpt2\"\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99a0686d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added alias pre_attn_norm for model.layers[*].pre_attn_norm -> input_layernorm in LlamaDecoderLayer\n",
      "Added alias eps for model.layers[*].pre_attn_norm.eps -> variance_epsilon in LlamaRMSNorm\n",
      "Added alias pre_mlp_norm for model.layers[*].pre_mlp_norm -> post_attention_layernorm in LlamaDecoderLayer\n"
     ]
    }
   ],
   "source": [
    "from hooked_transformer.auto_hooked_model import AutoHookedModelForCausalLM\n",
    "\n",
    "hooked_model = AutoHookedModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eae88e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"eos_token_id\": 128001,\n",
       "  \"head_dim\": 64,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8192,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 16,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 32.0,\n",
       "    \"high_freq_factor\": 4.0,\n",
       "    \"low_freq_factor\": 1.0,\n",
       "    \"original_max_position_embeddings\": 8192,\n",
       "    \"rope_type\": \"llama3\"\n",
       "  },\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.53.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 128256\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hooked_model.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "964d8a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embed_tokens': {'output': ['hidden_states']},\n",
       " 'layers': {'self': {'output': ['hidden_states']},\n",
       "  'pre_attn_norm': {'output': ['hidden_states']},\n",
       "  'self_attn': {'output': ['hidden_states', 'attn_weights']},\n",
       "  'q_proj': {'output': ['hidden_states']},\n",
       "  'k_proj': {'output': ['hidden_states']},\n",
       "  'v_proj': {'output': ['hidden_states']},\n",
       "  'pre_mlp_norm': {'output': ['hidden_states']},\n",
       "  'mlp': {'output': ['hidden_states']}},\n",
       " 'norm': {'output': ['hidden_states']},\n",
       " 'lm_head': {'output': ['logits']}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hooked_model.model.io_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e08c9ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_config = {\n",
    "    \"model.embed_tokens\": [\"out_hidden_states\", \"in_input\"],\n",
    "    \"lm_head\": [\"out_logits\", \"in_input\"],\n",
    "    \"model.norm\": [\"out_hidden_states\", \"in_hidden_states\"],\n",
    "    \"model.model.layers[0]\": [\"out_hidden_states\", \"in_hidden_states\"],\n",
    "    \"model.model.layers[0].pre_attn_norm\": [\"out_hidden_states\"],\n",
    "    \"model.model.layers[0].self_attn\": [\"out_hidden_states\", \"in_attention_mask\"],\n",
    "    \"model.model.layers[0].self_attn.q_proj\": [\"out_hidden_states\"],\n",
    "    \"model.model.layers[0].self_attn.k_proj\": [\"out_hidden_states\"],\n",
    "    \"model.model.layers[0].self_attn.v_proj\": [\"out_hidden_states\"],\n",
    "    \"model.model.layers[0].pre_mlp_norm\": [\"out_hidden_states\"],\n",
    "    \"model.model.layers[0].mlp\": [\"out_hidden_states\"],\n",
    "}\n",
    "\n",
    "if hooked_model.model.config.pos_type == \"ape\":\n",
    "    hook_config[\"model.embed_positions\"] = [\"out_hidden_states\", \"in_input\"]\n",
    "if hooked_model.model.config.pos_type == \"rope\":\n",
    "    hook_config[\"model.model.layers[0].self_attn\"].append(\"in_position_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fbba482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                                                                       <a href=\"file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logger.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py#78\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">78</span></a>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">08</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">07</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:08:19</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">619</span>/WARNING/hooked_transformer.hook/<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">warn_once</span><span style=\"font-weight: bold\">()</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">78</span>                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Hook at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Embedding</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128256</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span><span style=\"font-weight: bold\">)</span>                                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Inputs: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'in_input'</span><span style=\"font-weight: bold\">]</span>                                                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Outputs: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'out_hidden_states'</span><span style=\"font-weight: bold\">]</span>                                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                                                                                       \u001b]8;id=63962;file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=79749;file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py#78\u001b\\\u001b[2m78\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m08\u001b[0m-\u001b[1;36m07\u001b[0m \u001b[1;92m20:08:19\u001b[0m,\u001b[1;36m619\u001b[0m/WARNING/hooked_transformer.hook/\u001b[1;35mwarn_once\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m:\u001b[1;36m78\u001b[0m                                 \u001b[2m            \u001b[0m\n",
       "Hook at \u001b[1;35mEmbedding\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m128256\u001b[0m, \u001b[1;36m2048\u001b[0m\u001b[1m)\u001b[0m                                                                        \u001b[2m            \u001b[0m\n",
       "Inputs: \u001b[1m[\u001b[0m\u001b[32m'in_input'\u001b[0m\u001b[1m]\u001b[0m                                                                                   \u001b[2m            \u001b[0m\n",
       "Outputs: \u001b[1m[\u001b[0m\u001b[32m'out_hidden_states'\u001b[0m\u001b[1m]\u001b[0m                                                                         \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                                                                       <a href=\"file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logger.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py#78\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">78</span></a>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">08</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">07</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:08:19</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">743</span>/WARNING/hooked_transformer.hook/<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">warn_once</span><span style=\"font-weight: bold\">()</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">78</span>                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Hook at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128256</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Inputs: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'in_input'</span><span style=\"font-weight: bold\">]</span>                                                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Outputs: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'out_logits'</span><span style=\"font-weight: bold\">]</span>                                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                                                                                       \u001b]8;id=264521;file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=518753;file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py#78\u001b\\\u001b[2m78\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m08\u001b[0m-\u001b[1;36m07\u001b[0m \u001b[1;92m20:08:19\u001b[0m,\u001b[1;36m743\u001b[0m/WARNING/hooked_transformer.hook/\u001b[1;35mwarn_once\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m:\u001b[1;36m78\u001b[0m                                 \u001b[2m            \u001b[0m\n",
       "Hook at \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m128256\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                      \u001b[2m            \u001b[0m\n",
       "Inputs: \u001b[1m[\u001b[0m\u001b[32m'in_input'\u001b[0m\u001b[1m]\u001b[0m                                                                                   \u001b[2m            \u001b[0m\n",
       "Outputs: \u001b[1m[\u001b[0m\u001b[32m'out_logits'\u001b[0m\u001b[1m]\u001b[0m                                                                                \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                                                                       <a href=\"file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logger.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py#78\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">78</span></a>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">08</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">07</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:08:19</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">744</span>/WARNING/hooked_transformer.hook/<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">warn_once</span><span style=\"font-weight: bold\">()</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">78</span>                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Hook at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LlamaRMSNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-05</span><span style=\"font-weight: bold\">)</span>                                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Inputs: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'in_hidden_states'</span><span style=\"font-weight: bold\">]</span>                                                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Outputs: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'out_hidden_states'</span><span style=\"font-weight: bold\">]</span>                                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                                                                                       \u001b]8;id=245745;file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=767420;file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py#78\u001b\\\u001b[2m78\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m08\u001b[0m-\u001b[1;36m07\u001b[0m \u001b[1;92m20:08:19\u001b[0m,\u001b[1;36m744\u001b[0m/WARNING/hooked_transformer.hook/\u001b[1;35mwarn_once\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m:\u001b[1;36m78\u001b[0m                                 \u001b[2m            \u001b[0m\n",
       "Hook at \u001b[1;35mLlamaRMSNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2048\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m\u001b[1m)\u001b[0m                                                               \u001b[2m            \u001b[0m\n",
       "Inputs: \u001b[1m[\u001b[0m\u001b[32m'in_hidden_states'\u001b[0m\u001b[1m]\u001b[0m                                                                           \u001b[2m            \u001b[0m\n",
       "Outputs: \u001b[1m[\u001b[0m\u001b[32m'out_hidden_states'\u001b[0m\u001b[1m]\u001b[0m                                                                         \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'self': ['out_hidden_states', 'in_hidden_states'], 'pre_attn_norm': ['out_hidden_states'], 'self_attn': ['out_hidden_states', 'in_attention_mask', 'in_position_embeddings'], 'self_attn.q_proj': ['out_hidden_states'], 'self_attn.k_proj': ['out_hidden_states'], 'self_attn.v_proj': ['out_hidden_states'], 'pre_mlp_norm': ['out_hidden_states'], 'mlp': ['out_hidden_states']}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                                                                       <a href=\"file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logger.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py#78\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">78</span></a>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">08</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">07</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:08:19</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">745</span>/WARNING/hooked_transformer.hook/<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">warn_once</span><span style=\"font-weight: bold\">()</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">78</span>                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Hook at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LlamaDecoderLayer</span><span style=\"font-weight: bold\">(</span>                                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "  <span style=\"font-weight: bold\">(</span>self_attn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LlamaAttention</span><span style=\"font-weight: bold\">(</span>                                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "    <span style=\"font-weight: bold\">(</span>q_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "    <span style=\"font-weight: bold\">(</span>k_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "    <span style=\"font-weight: bold\">(</span>v_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "    <span style=\"font-weight: bold\">(</span>o_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "  <span style=\"font-weight: bold\">)</span>                                                                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "  <span style=\"font-weight: bold\">(</span>mlp<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LlamaMLP</span><span style=\"font-weight: bold\">(</span>                                                                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "    <span style=\"font-weight: bold\">(</span>gate_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "    <span style=\"font-weight: bold\">(</span>up_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "    <span style=\"font-weight: bold\">(</span>down_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "    <span style=\"font-weight: bold\">(</span>act_fn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SiLU</span><span style=\"font-weight: bold\">()</span>                                                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "  <span style=\"font-weight: bold\">)</span>                                                                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "  <span style=\"font-weight: bold\">(</span>input_layernorm<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LlamaRMSNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-05</span><span style=\"font-weight: bold\">)</span>                                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "  <span style=\"font-weight: bold\">(</span>post_attention_layernorm<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LlamaRMSNorm</span><span style=\"font-weight: bold\">((</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>,<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-05</span><span style=\"font-weight: bold\">)</span>                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"font-weight: bold\">)</span>                                                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Inputs: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'in_hidden_states'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'in_attention_mask'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'in_position_ids'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'in_past_key_value'</span>,              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'in_output_attentions'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'in_use_cache'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'in_cache_position'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'in_position_embeddings'</span><span style=\"font-weight: bold\">]</span>                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Outputs: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'out_hidden_states'</span><span style=\"font-weight: bold\">]</span>                                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                                                                                       \u001b]8;id=462198;file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=756365;file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py#78\u001b\\\u001b[2m78\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m08\u001b[0m-\u001b[1;36m07\u001b[0m \u001b[1;92m20:08:19\u001b[0m,\u001b[1;36m745\u001b[0m/WARNING/hooked_transformer.hook/\u001b[1;35mwarn_once\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m:\u001b[1;36m78\u001b[0m                                 \u001b[2m            \u001b[0m\n",
       "Hook at \u001b[1;35mLlamaDecoderLayer\u001b[0m\u001b[1m(\u001b[0m                                                                             \u001b[2m            \u001b[0m\n",
       "  \u001b[1m(\u001b[0mself_attn\u001b[1m)\u001b[0m: \u001b[1;35mLlamaAttention\u001b[0m\u001b[1m(\u001b[0m                                                                         \u001b[2m            \u001b[0m\n",
       "    \u001b[1m(\u001b[0mq_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                  \u001b[2m            \u001b[0m\n",
       "    \u001b[1m(\u001b[0mk_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                   \u001b[2m            \u001b[0m\n",
       "    \u001b[1m(\u001b[0mv_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                   \u001b[2m            \u001b[0m\n",
       "    \u001b[1m(\u001b[0mo_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                  \u001b[2m            \u001b[0m\n",
       "  \u001b[1m)\u001b[0m                                                                                                    \u001b[2m            \u001b[0m\n",
       "  \u001b[1m(\u001b[0mmlp\u001b[1m)\u001b[0m: \u001b[1;35mLlamaMLP\u001b[0m\u001b[1m(\u001b[0m                                                                                     \u001b[2m            \u001b[0m\n",
       "    \u001b[1m(\u001b[0mgate_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m8192\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                               \u001b[2m            \u001b[0m\n",
       "    \u001b[1m(\u001b[0mup_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m8192\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                 \u001b[2m            \u001b[0m\n",
       "    \u001b[1m(\u001b[0mdown_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m8192\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                               \u001b[2m            \u001b[0m\n",
       "    \u001b[1m(\u001b[0mact_fn\u001b[1m)\u001b[0m: \u001b[1;35mSiLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                                                                                   \u001b[2m            \u001b[0m\n",
       "  \u001b[1m)\u001b[0m                                                                                                    \u001b[2m            \u001b[0m\n",
       "  \u001b[1m(\u001b[0minput_layernorm\u001b[1m)\u001b[0m: \u001b[1;35mLlamaRMSNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2048\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m\u001b[1m)\u001b[0m                                                  \u001b[2m            \u001b[0m\n",
       "  \u001b[1m(\u001b[0mpost_attention_layernorm\u001b[1m)\u001b[0m: \u001b[1;35mLlamaRMSNorm\u001b[0m\u001b[1m(\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2048\u001b[0m,\u001b[1m)\u001b[0m, \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-05\u001b[0m\u001b[1m)\u001b[0m                                         \u001b[2m            \u001b[0m\n",
       "\u001b[1m)\u001b[0m                                                                                                      \u001b[2m            \u001b[0m\n",
       "Inputs: \u001b[1m[\u001b[0m\u001b[32m'in_hidden_states'\u001b[0m, \u001b[32m'in_attention_mask'\u001b[0m, \u001b[32m'in_position_ids'\u001b[0m, \u001b[32m'in_past_key_value'\u001b[0m,              \u001b[2m            \u001b[0m\n",
       "\u001b[32m'in_output_attentions'\u001b[0m, \u001b[32m'in_use_cache'\u001b[0m, \u001b[32m'in_cache_position'\u001b[0m, \u001b[32m'in_position_embeddings'\u001b[0m\u001b[1m]\u001b[0m                 \u001b[2m            \u001b[0m\n",
       "Outputs: \u001b[1m[\u001b[0m\u001b[32m'out_hidden_states'\u001b[0m\u001b[1m]\u001b[0m                                                                         \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                                                                       <a href=\"file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logger.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py#78\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">78</span></a>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">08</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">07</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:08:19</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">747</span>/WARNING/hooked_transformer.hook/<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">warn_once</span><span style=\"font-weight: bold\">()</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">78</span>                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Hook at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LlamaAttention</span><span style=\"font-weight: bold\">(</span>                                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "  <span style=\"font-weight: bold\">(</span>q_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "  <span style=\"font-weight: bold\">(</span>k_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "  <span style=\"font-weight: bold\">(</span>v_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "  <span style=\"font-weight: bold\">(</span>o_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"font-weight: bold\">)</span>                                                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Inputs: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'in_hidden_states'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'in_position_embeddings'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'in_attention_mask'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'in_past_key_value'</span>,       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'in_cache_position'</span><span style=\"font-weight: bold\">]</span>                                                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Outputs: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'out_hidden_states'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'out_attn_weights'</span><span style=\"font-weight: bold\">]</span>                                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                                                                                       \u001b]8;id=86383;file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=203103;file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py#78\u001b\\\u001b[2m78\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m08\u001b[0m-\u001b[1;36m07\u001b[0m \u001b[1;92m20:08:19\u001b[0m,\u001b[1;36m747\u001b[0m/WARNING/hooked_transformer.hook/\u001b[1;35mwarn_once\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m:\u001b[1;36m78\u001b[0m                                 \u001b[2m            \u001b[0m\n",
       "Hook at \u001b[1;35mLlamaAttention\u001b[0m\u001b[1m(\u001b[0m                                                                                \u001b[2m            \u001b[0m\n",
       "  \u001b[1m(\u001b[0mq_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                    \u001b[2m            \u001b[0m\n",
       "  \u001b[1m(\u001b[0mk_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                     \u001b[2m            \u001b[0m\n",
       "  \u001b[1m(\u001b[0mv_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                     \u001b[2m            \u001b[0m\n",
       "  \u001b[1m(\u001b[0mo_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                    \u001b[2m            \u001b[0m\n",
       "\u001b[1m)\u001b[0m                                                                                                      \u001b[2m            \u001b[0m\n",
       "Inputs: \u001b[1m[\u001b[0m\u001b[32m'in_hidden_states'\u001b[0m, \u001b[32m'in_position_embeddings'\u001b[0m, \u001b[32m'in_attention_mask'\u001b[0m, \u001b[32m'in_past_key_value'\u001b[0m,       \u001b[2m            \u001b[0m\n",
       "\u001b[32m'in_cache_position'\u001b[0m\u001b[1m]\u001b[0m                                                                                   \u001b[2m            \u001b[0m\n",
       "Outputs: \u001b[1m[\u001b[0m\u001b[32m'out_hidden_states'\u001b[0m, \u001b[32m'out_attn_weights'\u001b[0m\u001b[1m]\u001b[0m                                                     \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                                                                       <a href=\"file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logger.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py#78\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">78</span></a>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">08</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">07</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:08:19</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">748</span>/WARNING/hooked_transformer.hook/<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">warn_once</span><span style=\"font-weight: bold\">()</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">78</span>                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Hook at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LlamaMLP</span><span style=\"font-weight: bold\">(</span>                                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "  <span style=\"font-weight: bold\">(</span>gate_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "  <span style=\"font-weight: bold\">(</span>up_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "  <span style=\"font-weight: bold\">(</span>down_proj<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8192</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "  <span style=\"font-weight: bold\">(</span>act_fn<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SiLU</span><span style=\"font-weight: bold\">()</span>                                                                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"font-weight: bold\">)</span>                                                                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Inputs: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'in_x'</span><span style=\"font-weight: bold\">]</span>                                                                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Outputs: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'out_hidden_states'</span><span style=\"font-weight: bold\">]</span>                                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                                                                                       \u001b]8;id=286625;file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=424200;file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py#78\u001b\\\u001b[2m78\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m08\u001b[0m-\u001b[1;36m07\u001b[0m \u001b[1;92m20:08:19\u001b[0m,\u001b[1;36m748\u001b[0m/WARNING/hooked_transformer.hook/\u001b[1;35mwarn_once\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m:\u001b[1;36m78\u001b[0m                                 \u001b[2m            \u001b[0m\n",
       "Hook at \u001b[1;35mLlamaMLP\u001b[0m\u001b[1m(\u001b[0m                                                                                      \u001b[2m            \u001b[0m\n",
       "  \u001b[1m(\u001b[0mgate_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m8192\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                 \u001b[2m            \u001b[0m\n",
       "  \u001b[1m(\u001b[0mup_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m8192\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                   \u001b[2m            \u001b[0m\n",
       "  \u001b[1m(\u001b[0mdown_proj\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m8192\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                 \u001b[2m            \u001b[0m\n",
       "  \u001b[1m(\u001b[0mact_fn\u001b[1m)\u001b[0m: \u001b[1;35mSiLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m                                                                                     \u001b[2m            \u001b[0m\n",
       "\u001b[1m)\u001b[0m                                                                                                      \u001b[2m            \u001b[0m\n",
       "Inputs: \u001b[1m[\u001b[0m\u001b[32m'in_x'\u001b[0m\u001b[1m]\u001b[0m                                                                                       \u001b[2m            \u001b[0m\n",
       "Outputs: \u001b[1m[\u001b[0m\u001b[32m'out_hidden_states'\u001b[0m\u001b[1m]\u001b[0m                                                                         \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                                                                       <a href=\"file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logger.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py#78\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">78</span></a>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">08</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">07</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:08:19</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">749</span>/WARNING/hooked_transformer.hook/<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">warn_once</span><span style=\"font-weight: bold\">()</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">78</span>                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Hook at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Inputs: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'in_input'</span><span style=\"font-weight: bold\">]</span>                                                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Outputs: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'out_hidden_states'</span><span style=\"font-weight: bold\">]</span>                                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                                                                                       \u001b]8;id=340525;file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=755990;file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py#78\u001b\\\u001b[2m78\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m08\u001b[0m-\u001b[1;36m07\u001b[0m \u001b[1;92m20:08:19\u001b[0m,\u001b[1;36m749\u001b[0m/WARNING/hooked_transformer.hook/\u001b[1;35mwarn_once\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m:\u001b[1;36m78\u001b[0m                                 \u001b[2m            \u001b[0m\n",
       "Hook at \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                        \u001b[2m            \u001b[0m\n",
       "Inputs: \u001b[1m[\u001b[0m\u001b[32m'in_input'\u001b[0m\u001b[1m]\u001b[0m                                                                                   \u001b[2m            \u001b[0m\n",
       "Outputs: \u001b[1m[\u001b[0m\u001b[32m'out_hidden_states'\u001b[0m\u001b[1m]\u001b[0m                                                                         \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                                                                       <a href=\"file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logger.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py#78\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">78</span></a>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">08</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">07</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">20:08:19</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">750</span>/WARNING/hooked_transformer.hook/<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">warn_once</span><span style=\"font-weight: bold\">()</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">78</span>                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Hook at <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)</span>                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Inputs: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'in_input'</span><span style=\"font-weight: bold\">]</span>                                                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "Outputs: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'out_hidden_states'</span><span style=\"font-weight: bold\">]</span>                                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                                                                                       \u001b]8;id=915886;file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py\u001b\\\u001b[2mlogger.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=561314;file:///Users/238-gs20253502/Documents/codes/hooked-transformer/src/hooked_transformer/utils/logger.py#78\u001b\\\u001b[2m78\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m08\u001b[0m-\u001b[1;36m07\u001b[0m \u001b[1;92m20:08:19\u001b[0m,\u001b[1;36m750\u001b[0m/WARNING/hooked_transformer.hook/\u001b[1;35mwarn_once\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m:\u001b[1;36m78\u001b[0m                                 \u001b[2m            \u001b[0m\n",
       "Hook at \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m2048\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m                                         \u001b[2m            \u001b[0m\n",
       "Inputs: \u001b[1m[\u001b[0m\u001b[32m'in_input'\u001b[0m\u001b[1m]\u001b[0m                                                                                   \u001b[2m            \u001b[0m\n",
       "Outputs: \u001b[1m[\u001b[0m\u001b[32m'out_hidden_states'\u001b[0m\u001b[1m]\u001b[0m                                                                         \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hooked_model.register_hooks(hook_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75ceb55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"Tokyo is the capital of the\", \"Hello\"]\n",
    "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "generation_args = {\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "outputs = hooked_model(**inputs, **generation_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b912fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_LayerHookResult\n",
       "\tself: Hook\n",
       "\tresult: LayerHookResult\n",
       "\t\tin_hidden_states: torch.Size([2, 8, 2048])\n",
       "\t\tout_hidden_states: torch.Size([2, 8, 2048])\n",
       "\n",
       "\tpre_attn_norm: Hook\n",
       "\tresult: NormHookResult\n",
       "\t\tout_hidden_states: torch.Size([2, 8, 2048])\n",
       "\n",
       "\tself_attn: Hook\n",
       "\tresult: AttnHookResult\n",
       "\t\tin_position_embeddings: (torch.Size([1, 8, 64]), torch.Size([1, 8, 64]), )\n",
       "\t\tin_attention_mask: torch.Size([2, 1, 8, 8])\n",
       "\t\tout_hidden_states: torch.Size([2, 8, 2048])\n",
       "\n",
       "\tpre_mlp_norm: Hook\n",
       "\tresult: NormHookResult\n",
       "\t\tout_hidden_states: torch.Size([2, 8, 2048])\n",
       "\n",
       "\tmlp: Hook\n",
       "\tresult: MLPHookResult\n",
       "\t\tout_hidden_states: torch.Size([2, 8, 2048])\n",
       "\n",
       "\tself_attn_q_proj: Hook\n",
       "\tresult: AttnQProjHookResult\n",
       "\t\tout_hidden_states: torch.Size([2, 8, 2048])\n",
       "\n",
       "\tself_attn_k_proj: Hook\n",
       "\tresult: AttnKProjHookResult\n",
       "\t\tout_hidden_states: torch.Size([2, 8, 512])\n",
       "\n",
       "\tself_attn_v_proj: Hook\n",
       "\tresult: AttnVProjHookResult\n",
       "\t\tout_hidden_states: torch.Size([2, 8, 512])\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hooked_model.hooks.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50bc603d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMObservationBatchResult\n",
       "\tembed_tokens: EmbedTokensObservationBatchResult\n",
       "\tembed_positions: None\n",
       "\tlayers: LayerObservationBatchResult x 16\n",
       "\tnorm: NormObservationBatchResult\n",
       "\tlm_head: LMHeadObservationBatchResult"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = hooked_model.hook_results()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17ea1166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbedTokensObservationBatchResult\n",
       "\tinput_tokens: torch.Size([2, 8])\n",
       "\tout_hidden_states: torch.Size([2, 8, 2048])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0b1f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.embed_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89a67032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerObservationBatchResult\n",
       "\tpre_attn_norm_out: torch.Size([2, 8, 2048])\n",
       "\tattention_mask: torch.Size([2, 1, 8, 8])\n",
       "\tposition_embeddings: (torch.Size([1, 8, 64]), torch.Size([1, 8, 64]), )\n",
       "\tattn_out_hidden_states: torch.Size([2, 8, 2048])\n",
       "\tq_proj_output: torch.Size([2, 8, 2048])\n",
       "\tk_proj_output: torch.Size([2, 8, 512])\n",
       "\tv_proj_output: torch.Size([2, 8, 512])\n",
       "\tpre_mlp_norm_out: torch.Size([2, 8, 2048])\n",
       "\tmlp_out_hidden_states: torch.Size([2, 8, 2048])\n",
       "\tout_hidden_states: torch.Size([2, 8, 2048])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a1aebd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ True, False, False, False, False, False, False, False],\n",
       "          [ True,  True, False, False, False, False, False, False],\n",
       "          [ True,  True,  True, False, False, False, False, False],\n",
       "          [ True,  True,  True,  True, False, False, False, False],\n",
       "          [ True,  True,  True,  True,  True, False, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True, False, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True, False],\n",
       "          [ True,  True,  True,  True,  True,  True,  True,  True]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False,  True, False],\n",
       "          [False, False, False, False, False, False,  True,  True]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.layers[0].attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a1db2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state_reconstructed = (\n",
    "    result.embed_tokens.out_hidden_states\n",
    "    + result.layers[0].attn_out_hidden_states\n",
    "    + result.layers[0].mlp_out_hidden_states\n",
    ")\n",
    "\n",
    "if hooked_model.model.config.pos_type == \"ape\":\n",
    "    hidden_state_reconstructed += result.embed_positions.out_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b0860ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(\n",
    "    hidden_state_reconstructed, result.layers[0].out_hidden_states, atol=1e-5, rtol=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04c5e381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 64])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.layers[0].position_embeddings[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae24d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def attention(\n",
    "    query_states: torch.Tensor,\n",
    "    key_states: torch.Tensor,\n",
    "    value_states: torch.Tensor,\n",
    "    head_dim: int,\n",
    "    attention_mask: torch.Tensor,\n",
    "    o_proj: torch.nn.Module,\n",
    "    precompute_ov: bool = False,\n",
    "    rope: bool = False,\n",
    "    position_embeddings: torch.Tensor = None,\n",
    "):\n",
    "    if rope:\n",
    "        assert position_embeddings is not None, (\n",
    "            \"Position embeddings must be provided for RoPE\"\n",
    "        )\n",
    "    shape_q = (*query_states.shape[:-1], -1, head_dim)\n",
    "    shape_kv = (*key_states.shape[:-1], -1, head_dim)\n",
    "\n",
    "    query_states = query_states.view(shape_q).transpose(1, 2)\n",
    "    key_states = key_states.view(shape_kv).transpose(1, 2)\n",
    "\n",
    "    if rope:\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(\n",
    "            q=query_states,\n",
    "            k=key_states,\n",
    "            sin=sin,\n",
    "            cos=cos,\n",
    "        )\n",
    "\n",
    "    value_states = value_states.view(shape_kv).transpose(1, 2)\n",
    "\n",
    "    # prepare for gqa.\n",
    "    # can be done inside spda, but do it here for precompute_ov=True\n",
    "    batch_size, num_kheads, seq_length, _ = key_states.shape\n",
    "    _, num_qheads, _, head_dim = query_states.shape\n",
    "    key_states = repeat_kv(\n",
    "        key_states,\n",
    "        n_rep=num_qheads // num_kheads,\n",
    "    )\n",
    "    value_states = repeat_kv(\n",
    "        value_states,\n",
    "        n_rep=num_qheads // num_kheads,\n",
    "    )\n",
    "    if precompute_ov:\n",
    "        if hasattr(o_proj, \"li_weight\"):\n",
    "            weight = o_proj.li_weight\n",
    "        else:\n",
    "            weight = o_proj.weight.T\n",
    "        o_proj_by_head = weight.view(num_qheads, head_dim, -1)\n",
    "        value_states = torch.einsum(\"bhsi,hid->bhsd\", value_states, o_proj_by_head)\n",
    "\n",
    "    attn_weighted = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query=query_states,\n",
    "        key=key_states,\n",
    "        value=value_states,\n",
    "        attn_mask=attention_mask,\n",
    "    )\n",
    "\n",
    "    if precompute_ov:\n",
    "        if hasattr(o_proj, \"bias\") and o_proj.bias is not None:\n",
    "            return attn_weighted.sum(dim=1) + o_proj.bias\n",
    "        return attn_weighted.sum(dim=1)\n",
    "    attn_weighted = attn_weighted.transpose(1, 2).reshape(batch_size, seq_length, -1)\n",
    "    attn_weighted = o_proj(attn_weighted)\n",
    "    return attn_weighted\n",
    "\n",
    "\n",
    "torch.allclose(\n",
    "    attention(\n",
    "        query_states=result.layers[0].q_proj_output,\n",
    "        key_states=result.layers[0].k_proj_output,\n",
    "        value_states=result.layers[0].v_proj_output,\n",
    "        head_dim=hooked_model.model.config.hidden_size\n",
    "        // hooked_model.model.config.num_attention_heads,\n",
    "        attention_mask=result.layers[0].attention_mask,\n",
    "        o_proj=hooked_model.model.model.layers[0].self_attn.o_proj,\n",
    "        precompute_ov=True,\n",
    "        rope=hooked_model.model.config.pos_type == \"rope\",\n",
    "        position_embeddings=result.layers[0].position_embeddings\n",
    "        if hooked_model.model.config.pos_type == \"rope\"\n",
    "        else None,\n",
    "    )[:, -1, :],\n",
    "    result.layers[0].attn_out_hidden_states[:, -1, :],\n",
    "    atol=1e-5,\n",
    "    rtol=1e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f9003efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.7236e-03,  3.3289e-04, -5.5137e-02,  ...,  2.7408e-03,\n",
       "           2.7539e-03,  1.0591e-02],\n",
       "         [-1.3816e-05,  7.0120e-04,  9.2162e-03,  ...,  1.8067e-03,\n",
       "           8.1766e-03,  2.4078e-04]], grad_fn=<SliceBackward0>),)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    attention(\n",
    "        query_states=result.layers[0].q_proj_output,\n",
    "        key_states=result.layers[0].k_proj_output,\n",
    "        value_states=result.layers[0].v_proj_output,\n",
    "        head_dim=hooked_model.model.config.hidden_size\n",
    "        // hooked_model.model.config.num_attention_heads,\n",
    "        attention_mask=result.layers[0].attention_mask,\n",
    "        o_proj=hooked_model.model.model.layers[0].self_attn.o_proj,\n",
    "        precompute_ov=True,\n",
    "        rope=hooked_model.model.config.pos_type == \"rope\",\n",
    "        position_embeddings=result.layers[0].position_embeddings\n",
    "        if hooked_model.model.config.pos_type == \"rope\"\n",
    "        else None,\n",
    "    )[:, -1, :],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "162eeb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.7236e-03,  3.3289e-04, -5.5137e-02,  ...,  2.7408e-03,\n",
       "           2.7539e-03,  1.0591e-02],\n",
       "         [-1.3808e-05,  7.0120e-04,  9.2163e-03,  ...,  1.8067e-03,\n",
       "           8.1766e-03,  2.4078e-04]]),)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(result.layers[0].attn_out_hidden_states[:, -1, :],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85ede04b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaConfig' object has no attribute 'n_embd'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     29\u001b[39m     attn_weighted = torch.nn.functional.scaled_dot_product_attention(\n\u001b[32m     30\u001b[39m         query=query_states,\n\u001b[32m     31\u001b[39m         key=key_states,\n\u001b[32m     32\u001b[39m         value=value_states,\n\u001b[32m     33\u001b[39m         attn_mask=result.layers[\u001b[32m0\u001b[39m].attention_mask,\n\u001b[32m     34\u001b[39m     )\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_weighted.sum(dim=\u001b[32m1\u001b[39m) + o_proj.bias\n\u001b[32m     37\u001b[39m torch.allclose(\n\u001b[32m     38\u001b[39m     attention_precompute_ov(\n\u001b[32m     39\u001b[39m         query_states=result.layers[\u001b[32m0\u001b[39m].q_proj_output,\n\u001b[32m     40\u001b[39m         key_states=result.layers[\u001b[32m0\u001b[39m].k_proj_output,\n\u001b[32m     41\u001b[39m         value_states=result.layers[\u001b[32m0\u001b[39m].v_proj_output,\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m         head_dim=\u001b[43mhooked_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mn_embd\u001b[49m // hooked_model.model.config.n_head,\n\u001b[32m     43\u001b[39m         o_proj=hooked_model.model.model.layers[\u001b[32m0\u001b[39m].self_attn.c_proj,\n\u001b[32m     44\u001b[39m     )[:, -\u001b[32m1\u001b[39m, :], \n\u001b[32m     45\u001b[39m     result.layers[\u001b[32m0\u001b[39m].attn_out_hidden_states[:, -\u001b[32m1\u001b[39m, :],\n\u001b[32m     46\u001b[39m     rtol=\u001b[32m1e-04\u001b[39m,\n\u001b[32m     47\u001b[39m     atol=\u001b[32m1e-04\u001b[39m\n\u001b[32m     48\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/codes/hooked-transformer/.venv/lib/python3.13/site-packages/transformers/configuration_utils.py:209\u001b[39m, in \u001b[36mPretrainedConfig.__getattribute__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    208\u001b[39m     key = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m)[key]\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'LlamaConfig' object has no attribute 'n_embd'"
     ]
    }
   ],
   "source": [
    "def attention_precompute_ov(\n",
    "    query_states: torch.Tensor,\n",
    "    key_states: torch.Tensor,\n",
    "    value_states: torch.Tensor,\n",
    "    head_dim: int,\n",
    "    o_proj,\n",
    "):\n",
    "    shape_q = (*query_states.shape[:-1], -1, head_dim)\n",
    "    shape_kv = (*key_states.shape[:-1], -1, head_dim)\n",
    "\n",
    "    query_states = query_states.view(shape_q).transpose(1, 2)\n",
    "    key_states = key_states.view(shape_kv).transpose(1, 2)\n",
    "    value_states = value_states.view(shape_kv).transpose(1, 2)\n",
    "    batch_size, num_heads, seq_length, _ = query_states.shape\n",
    "\n",
    "    if hasattr(o_proj, \"li_weight\"):\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "    o_proj_by_head = o_proj.weight.view(num_heads, head_dim, -1)\n",
    "    value_states = torch.einsum(\"bhsi,hid->bhsd\", value_states, o_proj_by_head)\n",
    "\n",
    "    attn_weighted = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query=query_states,\n",
    "        key=key_states,\n",
    "        value=value_states,\n",
    "        attn_mask=result.layers[0].attention_mask,\n",
    "    )\n",
    "    return attn_weighted.sum(dim=1) + o_proj.bias\n",
    "\n",
    "\n",
    "torch.allclose(\n",
    "    attention_precompute_ov(\n",
    "        query_states=result.layers[0].q_proj_output,\n",
    "        key_states=result.layers[0].k_proj_output,\n",
    "        value_states=result.layers[0].v_proj_output,\n",
    "        head_dim=hooked_model.model.config.n_embd // hooked_model.model.config.n_head,\n",
    "        o_proj=hooked_model.model.model.layers[0].self_attn.c_proj,\n",
    "    )[:, -1, :],\n",
    "    result.layers[0].attn_out_hidden_states[:, -1, :],\n",
    "    rtol=1e-04,\n",
    "    atol=1e-04,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3c538a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2031,  0.0857,  0.1764,  ..., -0.0763,  0.0583, -0.1197],\n",
       "        [ 0.2686, -0.4273, -0.7308,  ...,  0.0217, -0.0387,  0.1454]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_precompute_ov(\n",
    "    query_states=result.layers[0].q_proj_output,\n",
    "    key_states=result.layers[0].k_proj_output,\n",
    "    value_states=result.layers[0].v_proj_output,\n",
    "    head_dim=hooked_model.model.config.n_embd // hooked_model.model.config.n_head,\n",
    "    o_proj=hooked_model.model.model.layers[0].self_attn.c_proj,\n",
    ")[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eedcec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.2031,  0.0857,  0.1764,  ..., -0.0763,  0.0583, -0.1197],\n",
       "         [ 0.2686, -0.4273, -0.7308,  ...,  0.0217, -0.0387,  0.1454]]),)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(result.layers[0].attn_out_hidden_states[:, -1, :],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebee7c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 768])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weghted = torch.nn.functional.scaled_dot_product_attention(\n",
    "    query=result.layers[0].q_proj_output,\n",
    "    key=result.layers[0].k_proj_output,\n",
    "    value=result.layers[0].v_proj_output,\n",
    ")\n",
    "attn_weghted.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hooked-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
